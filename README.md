<p align="left">
  <img src="https://github.com/user-attachments/assets/e093b2ec-4918-4622-a3c0-7d86d8e98db3" alt="ndna_logo_v5.jpg" width="1600" />
</p>

# <p align="center" target="_blank"> The Semantic Helix of Artificial Cognition <div align="center" target="_blank"> [![Webpage](https://img.shields.io/badge/üåê_Webpage-0078E7?style=for-the-badge&logo=google-chrome&logoColor=white)](https://pragyaai.github.io/ndna/llm/ndna/)   [![YouTube](https://img.shields.io/badge/YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white)](https://www.youtube.com/playlist?list=PLaE3N7Ax00P93HC0jxZ-b6c4tnLkldPOJ)   [![ArXiv](https://img.shields.io/badge/ArXiv-B31B1B?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.18216) </div> </p>

# nDNA

<img width="1100" height="950" alt="image" src="https://github.com/user-attachments/assets/ea0a1618-d6f9-4d77-8c14-ffd8ea31c546" />

This is a cumulative measure of latent geometry, quantifying how a large language model adapts its internal scaffolding to a given corpus. This framework establishes three fundamental metrics that collectively characterize the information processing landscape of Deep Neural Network in modern LLMs.<br>

**‚ñ∫ 1. _Spectral Curvature_ ($$Œ∫_{\ell}$$)**: Quantifies geometric properties of the parameter manifold <br><br>
**‚ñ∫ 2. _Thermodynamic Length_ ($$L_{\ell}$$)**: Measures information processing complexity via Fisher-Rao distances <br><br>
**‚ñ∫ 3. _Belief Vector Field_**: Captures epistemic confidence and uncertainty propagation <br><br>

The **Spectral Curvature (Œ∫‚Ñì)** capture geometry, **Thermodynamic length(L‚Ñì)** captures latent movement, and **Belief Vector Field** quantifies alignment strength of the model.

## ‚ñ∫ 1. Spectral Curvature ($$Œ∫_{\ell}$$) <br>

A ramp up of Power-Law exponent will increase the Latent Binding in higher decoder layers, signaling sharper representational semantics. Spectral Curvature at layer $\ell$ is defined as:

$$
\kappa_\ell := \big\| \Delta^2 h_\ell \big\| = \big\| h_{\ell+1} - 2 h_\ell + h_{\ell-1} \big\|
$$

In continuous form, this corresponds to:

$$
\kappa(s) = \left\| \frac{d^2 h(s)}{ds^2} \right\|
$$

where $s$ parameterizes depth through the network. Discrete $\kappa_\ell$ provides a practical, layerwise estimator.

<p align="center">
  <a href="[src="https://cdn.jsdelivr.net/gh/pragyaai/cdn-assets/assets/ndna/spectral_curvature_llama_ndna_animation.gif"](https://cdn.jsdelivr.net/gh/pragyaai/cdn-assets/assets/ndna/spectral_curvature_llama_ndna_animation.gif)">
    <img src="assets/spectral_curvature_llama_ndna_animation.gif" width="1000" height="850" />
  </a>
</p>

<p align="center">
  <a href="[https://pragyaai.github.io/ndna/thermodynamic_length.html]([https://cdn.jsdelivr.net/gh/pragyaai/cdn-assets/assets/ndna/spectral_curvature.gif](https://cdn.jsdelivr.net/gh/pragyaai/cdn-assets/assets/ndna/spectral_curvature.gif))">
    <img src="assets/spectral_curvature.gif" width="1000" height="850" />
  </a>
</p>

## ‚ñ∫ 2. Thermodynamic length ($$L_{\ell}$$)  <br> 

It is defined by Fisher geometry that quantifies semantic effort needed to move a token from layer($$\ell$$) to layer ($$\ell$$+1). Thermodynamic length offers a window onto the model‚Äôs "latent energy budget" ‚Äî illuminating how internal belief states reshape to meet complexity, constraint, and context.

---

**Formulation**  
Let $p_\ell(y|x)$ denote the model‚Äôs conditional distribution at layer $\ell$ given input $x$. The local epistemic cost is reflected in the squared norm of the gradient of log-likelihood with respect to model parameters:

$$
\big\| \nabla_\theta \log p_\ell(x) \big\|^2
$$

This quantity measures how much the model must *adjust its parameters locally* at layer $\ell$ to improve its fit to input $x$.

**Thermodynamic length at layer $\ell$ is defined as:**

$$
\mathcal{L}_\ell := \sum_{x \in \mathcal{D}} \big\| \nabla_\theta \log p_\ell(x) \big\|^2
= |\mathcal{D}| \, \mathbb{E}_{x \sim \mathcal{D}} \big\| \nabla_\theta \log p_\ell(x) \big\|^2
$$

This formulation reveals that $$L_\ell$$ captures both the *average local effort* and its scaling with dataset size. Here $$h_\ell$$ denotes latent trajectories at layer $\ell$, $$G_{\mathrm{Fisher}}$$ is the Fisher information metric, and $$s$$ is the arc length along $\gamma_\ell$. Thus, $\mathcal{L}_\ell$ can be seen as an *energy integral over the belief manifold* ‚Äî capturing how much *"heat"* or computational work is generated to reconcile prior belief state with new input at depth $\ell$.

<p align="center">
  <a href="[https://pragyaai.github.io/ndna/thermodynamics_interactive.html]([https://cdn.jsdelivr.net/gh/pragyaai/cdn-assets/assets/ndna/Final_thermodynamic_length.gif](https://cdn.jsdelivr.net/gh/pragyaai/cdn-assets/assets/ndna/Final_thermodynamic_length.gif))">
    <img src="assets/Final_thermodynamic_length.gif" width="1000" height="850" />
  </a>
</p>

<p align="center">
  <a href="[https://pragyaai.github.io/ndna/thermodynamic_length.html]([https://cdn.jsdelivr.net/gh/pragyaai/cdn-assets/assets/ndna/thermodynamic_length.gif](https://cdn.jsdelivr.net/gh/pragyaai/cdn-assets/assets/ndna/thermodynamic_length.gif))">
    <img src="assets/thermodynamic_length.gif" width="1000" height="850" />
  </a>
</p>

This formulation reveals that $$L_{\ell}$$ captures both the *average local effort* and its scaling with dataset size. Furthermore, in differential geometric terms, thermodynamic length can be written as a path energy:

$$
\mathcal{L}_{\ell} = \int_{\gamma_{\ell}} \left\langle \frac{d h_{\ell}}{d s}, \mathcal{G}_{\text{Fisher}}(h_{\ell}) \frac{d h_{\ell}}{d s} \right\rangle d s
$$

where $$h_{\ell}$$ denotes latent trajectories at layer $$\ell$$, $$G_{\text{Fisher}}$$ the Fisher information metric, and s arc length along $$\gamma_{\ell}$$. Thus, $$L_{\ell}$$ can be seen as an *energy integral over the belief manifold* ‚Äì capturing how much internal *"heat"* or computational work is generated to reconcile prior belief state with new input at depth $$\ell$$.


It quantifies the epistemic work performed across transformer layers, calculated as the cumulative squared gradient norm of layerwise log-likelihoods. Higher values signal internal resistance‚Äìzones of significant restructuring, belief compression, or negotiation of conflicting priors. In culturally fine-tuned models, these peaks localize to upper decoder layers, indicating intense adaptation near output-generating blocks. Within the nDNA construct, **$$L_{\ell}$$** helps reveal latent epistemic effort that underlies surface-level behavior. This metric thus provides a nuanced window into where and how models internally allocate effort during learning and inference. <br><br>

## ‚ñ∫ 3. Belief Vector Field  <br>
In **differential geometry** and **physics**, a *vector field* describes a directional force applied at each point of a space. Inspired by this, the **Belief Vector Field** models the *directional semantic force* that a specific culture or value system exerts on a model‚Äôs latent representations. It encodes *where*, *how strongly*, and *in what direction* cultural priors act within the model‚Äôs internal geometry‚Äîfunctioning as a **semantic compass** through the latent manifold.<br>

<p align="center">
  <a href="[https://pragyaai.github.io/ndna/belief_vector_field.html]([https://cdn.jsdelivr.net/gh/pragyaai/cdn-assets/assets/ndna/belief_vector_field.gif](https://cdn.jsdelivr.net/gh/pragyaai/cdn-assets/assets/ndna/belief_vector_field.gif))">
    <img src="assets/Final_belief_vector_field.gif" width="1100" height="900" />
  </a>
</p>

<p align="center">
  <img src="assets/belief_vector_field_healthy_static_annotated.png" width="900" height="1200" alt="Belief Vector Field" />
</p>

# The Fingerprint

<img width="1276" height="663" alt="image" src="https://github.com/user-attachments/assets/a4be9afd-cac8-4aba-ae3a-10216dacb4a0" />

<p align="center">
  <a href="[https://pragyaai.github.io/ndna/thermodynamic_length.html]([https://cdn.jsdelivr.net/gh/pragyaai/cdn-assets/assets/ndna/ndna_refined_story.gif](https://cdn.jsdelivr.net/gh/pragyaai/cdn-assets/assets/ndna/ndna_refined_story.gif))">
    <img src="assets/ndna_refined_story.gif" width="1200" height="950" />
  </a>
</p>


# Datasets

‚ñ∫ For **Spectral Culvature** and **Thermodynamic length** we have used **[SQuAD 2.0](https://huggingface.co/datasets/rajpurkar/squad_v2)** <br><br>
‚ñ∫ For **Belief Vector Field**, we have used less concrete words [Dataset Brysbaert 2014 Concreteness](https://norare.clld.org/contributions/Brysbaert-2014-Concreteness)    [[1](https://norare.clld.org/contributions/Brysbaert-2014-Concreteness)   [2](https://link.springer.com/article/10.3758/s13428-013-0403-5)]

# Models

Throughout our work, we used the following foundational LLMs to prove our hypothesis:

## **‚ñ∫ Dense transformers**  
##### 1. [LLaMA-2 base](https://huggingface.co/docs/transformers/en/model_doc/llama2)  
##### 2. [LLaMA-2 instruct](https://huggingface.co/upstage/Llama-2-70b-instruct)  
##### 3. [LLaMA-3 3B base](https://huggingface.co/meta-llama/Llama-3.2-3B)  
##### 4. [LLaMA-3 3B instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)  
##### 5. [Gemma 7B](https://huggingface.co/google/gemma-7b)  
##### 6. [Falcon](https://huggingface.co/docs/transformers/en/model_doc/falcon#transformers.FalconModel)  
##### 7. [GPT-2](https://huggingface.co/openai-community/gpt2)  
##### 8. [GPT-NeoX](https://huggingface.co/EleutherAI/gpt-neox-20b)  
##### 9. [DialoGPT-medium](https://huggingface.co/microsoft/DialoGPT-medium)  

## **‚ñ∫ Sparse mixture-of-expert**  
##### 1. [Mixtral expert variants](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)  

## **‚ñ∫ Multilingual and culturally calibrated models**  
##### 1. [Qwen2.5 base](https://huggingface.co/Qwen/Qwen2.5-72B)  
##### 2. [Qwen2.5 instruct](https://huggingface.co/unsloth/Qwen2.5-7B-Instruct)  

## **‚ñ∫ Compact efficient architectures**  
##### 1. [Phi-2 base](https://huggingface.co/microsoft/phi-2)  
##### 2. [Phi-2 instruct](https://huggingface.co/venkycs/phi-2-instruct)  
##### 3. [TinyLLama](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)  

<p align="center">
  <a href="[https://pragyaai.github.io/ndna/belief_vector_field.html]([https://cdn.jsdelivr.net/gh/pragyaai/cdn-assets/assets/gifs/15_llms/all.gif](https://cdn.jsdelivr.net/gh/pragyaai/cdn-assets/assets/gifs/15_llms/all.gif))">
    <img src="assets/all_LLms.gif" width="1200" height="1000" />
  </a>
</p>

# Installation Instructions

Transformers works with [Python 3.9+](https://www.python.org/downloads/), and [PyTorch 2.1+](https://pytorch.org/get-started/locally/) <br>

Create and activate a virtual environment with **virtualenv(venv)** or **uv**, a fast Rust-based Python package and project manager.<br>
 
```py
# venv
python -m venv .my-env
source .my-env/bin/activate
# uv
uv venv .my-env
source .my-env/bin/activate
```

Install Transformers in your virtual environment.

```py
# pip
pip install "transformers[torch]"

# uv
uv pip install "transformers[torch]"
```

```py
import torch
from transformers import pipeline
```

# Install from source

```shell
git clone [https://github.com/triton-lang/triton.git](https://github.com/pragyaai/nDNA-v1.git)
cd triton

pip install -r python/requirements.txt # build-time dependencies
pip install -e .
```

Or with a virtualenv:

```shell
git clone [https://github.com/triton-lang/triton.git](https://github.com/pragyaai/nDNA-v1.git)
cd nDNA-v1

python -m venv .venv --prompt nDNA-v1
source .venv/bin/activate

pip install -r python/requirements.txt # build-time dependencies
pip install -e .

pip install torch>=2.1 "transformers[torch]" matplotlib datasets pandas seaborn numpy tqdm
```
Or from the project requirements file:

```shell
pip install -r requirements.txt
```

Verify your setup
```shell
python -m nDNA-v1.test_install
```

(Optional) If you plan to modify or extend the codebase, install in editable developer mode.
```shell
git clone https://github.com/pragyaai/nDNA-v1.git
cd nDNA-v1

# Create virtual environment
python -m venv .venv --prompt nDNA-v1
source .venv/bin/activate

# Install build-time and runtime dependencies
pip install -r requirements.txt
pip install -e .
```

If installation succeeded, you‚Äôll see following message:
##### nDNA environment ready and components loaded successfully.
---

## Citation

If you use nDNA in your research, please cite:

```bibtex
@misc{das2025ndnasemantichelix,
  title={nDNA -- the Semantic Helix of Artificial Cognition}, 
  author={Amitava Das},
  year={2025},
  eprint={2509.18216},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2509.18216}
}
